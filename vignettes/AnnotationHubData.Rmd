<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Annotation}
-->

```{r setup, echo=FALSE}
library(knitr)
options(width=80)
```
```{r wrap-hook, echo=FALSE}
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```


# The AnnotationHubData Package

### by Paul Shannon and Marc Carlson

## Overview

The AnnotationHubData package provides tools to acquire,
annotate, convert and store data for use in Bioconductor's
AnnotationHub. BED files from the Encode project, gtf
files from Ensembl, or annotation tracks from UCSC, are
examples of data that can be downloaded, described with metadata,
transformed to standard Bioconductor data types, and stored so that
they may be conveniently served up on demand to users via the
AnnotationHub client.

For a description of how to put new data into the AnnotationHub,
please see the vignette titled "How to write recipes for new resources
for the AnnotationHub" in the AnnotationHub package.

This document will deal with the nuts and bolts of how the back end of
the AnnotationHub works and operates.

## How is the metadata stored?

When a recipe is run a lot of metadata is also required and collected
about the data that is processed by that recipe.  This data is stored
as a record in a MySQL database back end.  When the user runs their
client, this database is downloaded (if needed) as a sqlite dump of
that MySQL backend.  Performance benefits from using MySQL come into
play whenever we need to upload new records into the back end, but it
was also decided that the portability of a sqlite DB is better for
local cached access, so both kinds of databases are used in this
implementation.  But the 'parent' database is the MySQL db (the sqlite
ones are just cached copies of that).

Normally when we work with this database we will 1st do testing on a
local instance on gamay and then update the production machine later.

To log in to the database on gamay use this (you will need the password):
\begin{quote}
  mysql -p -u ahuser annotationhub
\end{quote}

\begin{verbatim}
  mysql -p -u ahuser annotationhub
\end{verbatim}

And for production you will need to connect here:
% <<eval=FALSE>>=
% #  ssh ubuntu@annotationhub.bioconductor.org
% @ 

And then connect to the database:
% <<eval=FALSE>>=
% #  mysql -p -u ahuser annotationhub
% @

Whenever you have updated the MySQL back end, a chron job should do
the job of creating the locally cached sqlite database from the MySQL
source db.  This is done by a ruby script called convert_db.rb.  This
file in part fo code that is checked in at the following URL in
github.

% <<eval=FALSE>>=
% #  https://github.com/Bioconductor/AnnotationHubServer3.0
% @ 



\subsection{Where is the data stored?}

Unlike the metadata, the data that it stored after a recipe is run is
stored in something that looks like a file system. When testing on
gamay this data is stored in /var/FastRWeb/, but our production server
stores and retrieves data from S3.  So to get the data moved up to
production you need to have access to these S3 resources.



To copy things up to the S3 buckets you need to use the aws command
line tools. You can see help like this
% <<eval=FALSE>>=
% # aws s3 help
% @ 

And you can get "copying" help like this
% <<eval=FALSE>>=
% # aws s3 cp help
% @ 

And you can copy something to be read only by using the --acl argument
like this:
% <<eval=FALSE>>=
% # aws s3 cp --acl public-read hello.txt 
% s3://annotationhub/foo/bar/hello.txt
% @ 


To sign in at the AWS console go here: (use your username and password
the account is bioconductor)
% <<eval=FALSE>>=
% # https://bioconductor.signin.aws.amazon.com/console
% @ 
And then click 'S3', 'annotationhub' etc.

And to recursively copy back down from the S3 bucket do like this:
% <<eval=FALSE>>=
% #  aws s3 cp --dryrun --recursive s3://annotationhub/ensembl/release-75/fasta/ .
% @ 

And then to copy back up you could do it like this:
% <<eval=FALSE>>=
% #  aws s3 cp --dryrun --recursive --acl public-read ./fasta/ s3://annotationhub/ensembl/release-75/fasta/@ 
% @ 

But that will copy everything (not very efficient since some things
may be the same and already there) So to actually copy back we should
really use 'aws s3 sync' instead of 'aws s3 cp'.
% <<eval=FALSE>>=
% #  aws s3 sync help
% @ 





